---
title: "Data Cleaning"
bibliography: reference.bib
editor_options: 
  chunk_output_type: inline
---

# Data

Data to evaluate NFL decision making was gathered by using the nflreadr package [@nflverse]. The creators of this package provide detailed instructions and examples on how to use and clean this data, which can be found at their website [here](https://nflreadr.nflverse.com/).

# Cleaning Process

## 4th Down Data

```{r setup, include = FALSE}
library(nflverse)
library(tidyverse)
library(ggplot2)
library(grid)
library(reticulate)
library(kableExtra)
use_condaenv("r-env")
options(scipen = 999)
```

```{r load_data, message = FALSE}
raw_data <- load_4th_pbp(2022)
```

```{r head, message = FALSE}
head(raw_data)
```

There are a large number of columns in this data, taken from the 2022 NFL season, we do not need all 383. The purpose of this data is to help determine how coaches today approach 4th down, a basic yet essential decision in the NFL. To do this we only need to look at these columns:

```{r, message = FALSE}
fourth_downs <- raw_data %>%
    filter(!is.na(go_boost) & !is.na(go)) %>%
    select(home_coach, away_coach, posteam_type, ydstogo, yardline_100, posteam, go_boost, go, epa)

head(fourth_downs)
```

Each unit in this table is one 4th down decision. The variables include:

```{r, echo = FALSE}
names <- data.frame("Variable" = names(fourth_downs),
                    "Definition" = c("Home Team Coach",
                                     "Away Team Coach",
                                     "Offense Home or Away",
                                     "Yards to Go",
                                     "Yardline Relative to End Zone",
                                     "Offense Team",
                                     "Change in Win Probability if Conversion Attempted",
                                     "Conversion Attempted (T/F)",
                                     "Estimated Points Added as a Result of the Play"))

kable(names)

```

```{r}
fourth_decisions <- fourth_downs %>%
    mutate(should_go = if_else((go == 100 & go_boost > 0), 1, (if_else(go_boost < 0, -1, 0))),
           shouldnt_go = if_else((go == 0 & go_boost < 0), 1, (if_else(go_boost > 0, -1, 0))),
           coach = if_else(posteam_type == "home", home_coach, away_coach),) %>%
    select(coach, ydstogo, yardline_100, go_boost, should_go, shouldnt_go, go, epa) %>%
    group_by(coach) %>%
    summarize(should_go = sum(should_go == 1) / (sum(should_go == 0 | should_go == 1)),
              shouldnt_go = sum(shouldnt_go == 1) / (sum(shouldnt_go == 0 | shouldnt_go == 1)),
              EPA = mean(epa),
              count = n()) %>%
    filter(count > 50)


head(fourth_decisions)
```

This data shows when coaches make the correct choice on 4th down, their average estimated points added on 4th downs throughout the season, and the number of 4th down decisions they had to make throughout the season.

## Sentiment Analysis

The NFL subreddit is a useful place to extract text data produced by fans and their reaction to these essential decisions. Like most jobs, the goal of an NFL head coach is not only to win games, but to keep their bosses happy. The size and outspoken nature of NFL fan bases can be a factor in the job security of these coaches, as if a coach loses the fans, their job gets a lot harder. This is where sentiment analysis comes in, as getting an idea of how fans feel about 4th down decisions by certain coaches may give us insights into how they are making their decisions, and whether or not the fans have any influence on the decision making process.

```{python}
import praw
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np

reddit = praw.Reddit("bot1")
```

This allows us to get posts from the NFL subreddit using a search query of "4th down" and access the comments of those posts.

```{python, warning = FALSE}
subreddit = reddit.subreddit("nfl")
query = "4th down"

top_posts = subreddit.search(query, limit=100)
urls = []
corpus = []

for post in top_posts:
    urls.append(post.permalink)
    
url = "https://www.reddit.com" + urls[0]

submission = reddit.submission(url=url)

for top_level_comment in submission.comments:
    corpus.append(top_level_comment.body)

vectorizer=CountVectorizer()
Xs = vectorizer.fit_transform(corpus)

col_names=vectorizer.get_feature_names_out()
```

The overall sentiment from this brief analysis is a positive one, but the search terms need to be more specific to get any useful information out of this process.

```{python}
from nltk.sentiment import SentimentIntensityAnalyzer

overall = []
sia = SentimentIntensityAnalyzer()

for text in corpus:
  score=sia.polarity_scores(text)
  overall.append(score['compound'])



sum(overall)/len(overall)
```
