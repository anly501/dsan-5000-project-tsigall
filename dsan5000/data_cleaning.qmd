---
title: "Data Cleaning"
bibliography: reference.bib
editor_options: 
  chunk_output_type: inline
---

# Cleaning Process

## 4th Down Data

### By Coach

```{r setup, include = FALSE}
library(nflverse)
library(tidyverse)
library(ggplot2)
library(grid)
library(reticulate)
library(kableExtra)
use_condaenv("r-env")
options(scipen = 999)
```

```{r load_data, include = FALSE}
load("raw_nfl.Rdata")
```

```{r head, message = FALSE}
head(raw_data)
```

There are a large number of columns in this data, taken from the 2022 NFL season, we do not need all 383. The purpose of this data is to help determine how coaches today approach 4th down, a basic yet essential decision in the NFL. To do this we only need to look at these columns:

```{r, message = FALSE}
fourth_downs <- raw_data %>%
    filter(!is.na(go_boost) & !is.na(go)) %>%
    select(home_coach, away_coach, posteam_type, ydstogo, yardline_100, posteam, go_boost, go, epa)

head(fourth_downs)
```

Each unit in this table is one 4th down decision. The variables include:

```{r, echo = FALSE}
names <- data.frame("Variable" = names(fourth_downs),
                    "Definition" = c("Home Team Coach",
                                     "Away Team Coach",
                                     "Offense Home or Away",
                                     "Yards to Go",
                                     "Yardline Relative to End Zone",
                                     "Offense Team",
                                     "Change in Win Probability if Conversion Attempted",
                                     "Conversion Attempted (T/F)",
                                     "Estimated Points Added as a Result of the Play"))

kable(names)

```

Making a table showing when coaches make the correct decisions on 4th down is essential in evaluating their decision making process. This is made easy by the inclusion of the `go_boost` variable.

```{r}
fourth_decisions <- fourth_downs %>%
    mutate(should_go = ifelse((go_boost > 0),
                             (if_else(go == 100, 1, 0)),
                             NA),
          shouldnt_go = ifelse((go_boost < 0),
                             (if_else(go == 0, 1, 0)),
                             NA),
           coach = if_else(posteam_type == "home", home_coach, away_coach),) %>%
    select(coach, ydstogo, yardline_100, go_boost, should_go, shouldnt_go, go, epa) %>%
    group_by(coach) %>%
    summarize(should_go = 
                sum(should_go == 1, na.rm = TRUE) / 
                (sum(should_go == 0 | should_go == 1, na.rm = TRUE)),
              shouldnt_go = 
                sum(shouldnt_go == 1, na.rm = TRUE) / 
                (sum(shouldnt_go == 0 | shouldnt_go == 1, na.rm = TRUE)),
              EPA = mean(epa),
              count = n()) %>%
    filter(count > 50)


head(fourth_decisions)
```

This data shows when coaches make the correct choice on 4th down, their average estimated points added on 4th downs throughout the season, and the number of 4th down decisions they had to make throughout the season. It shows the percent of times each coach makes the correct decision in two scenarios: when going for it would have a positive effect on their win probability and when going for it would have a negative effect on their win probability.

### By Situation

One of, if not the most important factor in making a decision on fourth down is the position your team is on the field. Looking at this more closely requires a table that shows coaches decisions based on where they are on the field and how many downs they need to gain to get a first down.

```{r by_position}
fourth_position <- fourth_downs %>%
  mutate(should_go = ifelse((go_boost > 0),
                             (if_else(go == 100, 1, 0)),
                             NA),
         shouldnt_go = ifelse((go_boost < 0),
                             (if_else(go == 0, 1, 0)),
                             NA)) %>%
  select(yardline_100,
         ydstogo,
         go,
         go_boost,
         should_go,
         shouldnt_go) %>%
  group_by(yardline_100, ydstogo) %>%
  summarize(go = mean(go),
            count = n(),
            go_boost = mean(go_boost)) %>%
  filter(go_boost > -20)

head(fourth_position)

```

This table shows us the percentage of times a coach chose to go for it in the given situation. The first row, for example, shows that 80.5% of the time a coach decided to go for it when they were at 4th and 1 at the 1 yard line - they were 1 yard away from scoring. It also shows is the average increase in win probability if a coach were to go for it. For example, at 4th and 1 at the 1 yard line, going for it increases your win probability by an average of 7%, an incredibly large number.

## Sentiment Analysis

The NFL subreddit is a useful place to extract text data produced by fans and their reaction to these essential decisions. Like most jobs, the goal of an NFL head coach is not only to win games, but to keep their bosses happy. The size and outspoken nature of NFL fan bases can be a factor in the job security of these coaches, as if a coach loses the fans, their job gets a lot harder. This is where sentiment analysis comes in, as getting an idea of how fans feel about 4th down decisions by certain coaches may give us insights into how they are making their decisions, and whether or not the fans have any influence on the decision making process.

```{python}
import praw
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np

reddit = praw.Reddit("bot1")
```

This allows us to get posts from the NFL subreddit using a search query of "4th down" and access the comments of those posts.

```{python, warning = FALSE}
subreddit = reddit.subreddit("nfl")
query = "4th down"

top_posts = subreddit.search(query, limit=100)
urls = []
corpus = []

for post in top_posts:
    urls.append(post.permalink)
    
url = "https://www.reddit.com" + urls[0]

submission = reddit.submission(url=url)

submission.comments.replace_more(limit=0)
for top_level_comment in submission.comments:
    corpus.append(top_level_comment.body)

vectorizer=CountVectorizer()
Xs = vectorizer.fit_transform(corpus)

col_names=vectorizer.get_feature_names_out()
```

The overall sentiment from this brief analysis is a positive one, but the search terms need to be more specific to get any useful information out of this process.

```{python}
from nltk.sentiment import SentimentIntensityAnalyzer

overall = []
sia = SentimentIntensityAnalyzer()

for text in corpus:
  score=sia.polarity_scores(text)
  overall.append(score['compound'])



sum(overall)/len(overall)
```

```{r include = FALSE}
save(fourth_decisions, fourth_downs, fourth_position, file = "clean_data.Rdata")
```
