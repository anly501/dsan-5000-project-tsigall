---
title: "Data Cleaning"
bibliography: reference.bib
editor_options: 
  chunk_output_type: inline
---

# Data

Data to evaluate NFL decision making was gathered by using the nflreadr package [@nflverse]. The creators of this package provide detailed instructions and examples on how to use and clean this data, which can be found at their website [here](https://nflreadr.nflverse.com/).

# Cleaning Process

## 4th Down Data

```{r setup, include = FALSE}
library(nflverse)
library(tidyverse)
library(ggplot2)
library(grid)
options(scipen = 999)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

```{r load_data}
raw_data <- load_4th_pbp(2022)
```

```{r head, message = FALSE}
head(raw_data)
```

There are a large number of columns in this data, taken from the 2022 NFL season, we do not need all 383. The purpose of this data is to help determine how coaches today approach 4th down, a basic yet essential decision in the NFL. To do this we only need to look at these columns:

```{r, message = FALSE}
fourth_downs <- raw_data %>%
    filter(!is.na(go_boost) & !is.na(go)) %>%
    select(home_coach, away_coach, posteam_type, ydstogo, yardline_100, posteam, go_boost, go, epa)

head(fourth_downs)
```

Each unit in this table is one 4th down decision. The variables include:

```{r, echo = FALSE}
names <- data.frame("Variable" = names(fourth_downs),
                    "Definition" = c("Home Team Coach",
                                     "Away Team Coach",
                                     "Offense Home or Away",
                                     "Yards to Go",
                                     "Yardline Relative to End Zone",
                                     "Offense Team",
                                     "Change in Win Probability if Conversion Attempted",
                                     "Conversion Attempted (T/F)",
                                     "Estimated Points Added as a Result of the Play"))

rmarkdown::paged_table(names)

```

```{r}
fourth_decisions <- fourth_downs %>%
    mutate(should_go = if_else((go == 100 & go_boost > 0), 1, (if_else(go_boost < 0, -1, 0))),
           shouldnt_go = if_else((go == 0 & go_boost < 0), 1, (if_else(go_boost > 0, -1, 0))),
           coach = if_else(posteam_type == "home", home_coach, away_coach),) %>%
    select(coach, ydstogo, yardline_100, go_boost, should_go, shouldnt_go, go, epa) %>%
    group_by(coach) %>%
    summarize(should_go = sum(should_go == 1) / (sum(should_go == 0 | should_go == 1)),
              shouldnt_go = sum(shouldnt_go == 1) / (sum(shouldnt_go == 0 | shouldnt_go == 1)),
              EPA = mean(epa),
              count = n()) %>%
    filter(count > 50)


rmarkdown::paged_table(head(fourth_decisions))
```

This data shows when coaches make the correct choice on 4th down, their average estimated points added on 4th downs throughout the season, and the number of 4th down decisions they had to make throughout the season.

## Text Data

2.  Have Python code that cleans text data and includes a label (from corpus or csv or both) and you MUST use CountVectorizer.

```{python}
print("hello world")
```

(Optional but recommended) Have R code that cleans text data from a corpus for your project. (Optional but recommended) Have Python code that cleans record data for your project.

Host your cleaning codes and data in the GitHub repository.

Get your data into a cleaner state that will enable you to further (and later) prepare it for modeling.
