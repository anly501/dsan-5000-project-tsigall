---
title: "Clustering"
format:
  html:
    code-fold: true
    code-summary: "Show the code"
---

```{r setup, include = FALSE}
library(tidyverse)
library(plotly)
library(reticulate)
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

Attempting to "profile" each coach by how they perform on 4th down may give us some important insights into how they make their decisions. This would be a good fit for an unsupervised learning task, as this is just some exploratory analysis to see if we can extract some trends from this data. We do not know yet what kinds of coaches there may be out there, so lets attempt to group them based on the data we do have.

# Theory

## K-Means

K-Means clustering is a centroid based clustering algorithm that aims to categorize each observation into the nearest cluster out of the total number k. To do this it randomly picks k points, then computes the distance from each point to each observation. Each observation is assigned the centroid it is closest to, then the centroids are recalculated based on these assignments and the process repeats. This repeats until it is stationary.

## DBSCAN

DBSCAN is a density based clustering algorithm that groups points together based on how close they are to their neighbors. Core points are determined, those being ones that have a certain amount of points close by. Other observations are assigned to the same cluster as those core points if they are within a certain distance of that point. The process repeats until stationary. As a result, outliers belong their own clusters as they are not close to any other points.

## Agglomerative

Agglomerative clustering is a hierarchical clustering algorithm that initially treats each point as its own cluster. The distance between each observation is calculated, and the two closest points are determined to be a cluster. They are merged and the distance matrix is updated with one less cluster (as two clusters (points) were just merged). The two closest clusters are then determined and merged, and this repeats until some stopping criterion is met.

# Methods

## Clean Data for Clustering

We want to select the features to be used during this process, lets try just using two features to start with: `should_go` and `shouldnt_go` from our earlier cleaning process. Lets also group by coach alone and not by coach and season to make it easier to interpret the final output.

```{r clean_data}
load("data/clean_data.Rdata")

fourth_by_coach <- fourth_decisions %>% 
  group_by(coach) %>%
  summarize(should_go = mean(should_go),
            shouldnt_go = mean(shouldnt_go)) %>%
  ungroup()
  
head(fourth_by_coach)
```

Lets get an initial look at the data before applying the clustering algorithm.

```{r initial_viz}
plot_ly(data = fourth_by_coach, x = ~should_go, y = ~shouldnt_go, type = "scatter", mode = "markers")
```

Obvious clusters have not formed from this, but we will still proceed with the unsupervised learning task.

```{r to_csv, include = FALSE}
write_csv(fourth_by_coach, file = "data/fourth_by_coach.csv")
```

### Imports

```{python imports}
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import sklearn

np.random.seed(621)
```

```{python read_data, include = FALSE}
df = pd.read_csv("data/fourth_by_coach.csv")
scaler = StandardScaler()
X = scaler.fit_transform(df[["should_go", "shouldnt_go"]])
```

### Plotting Function

```{python plot_function}
# plotting function from lab 4.1 demo
def plot(X,color_vector):
    fig, ax = plt.subplots()
    ax.scatter(X[:,0], X[:,1],c=color_vector, alpha=0.5) #, c=y
    ax.set(xlabel='Feature-1 (x_1)', ylabel='Feature-2 (x_2)',
    title='Cluster data')
    ax.grid()
    # fig.savefig("test.png")
    plt.show()
```

## K-Means

```{python clustering, messages = FALSE}
X = np.ascontiguousarray(X)
k_means_X = pd.DataFrame(columns = ["Cluster", "Inertia"], index = range(10))

for i in range(1,11):
  model = KMeans(n_clusters = i, n_init = 10).fit(X)
  k_means_X.at[i - 1, "Inertia"] = model.inertia_
  k_means_X.at[i - 1, "Cluster"] = i
  

plt.clf()
sns.lineplot(data = k_means_X, x = "Cluster", y = "Inertia")
plt.show()
```

Using the elbow method, the correct number of clusters here seems to be about 3, we will try 2, 3, and 4.

::: panel-tabset
### 2 Clusters

```{python plot_clusters2, messages = FALSE}
labels = KMeans(n_clusters = 2, n_init = 10).fit(X).labels_

plot(X, labels)
```

### 3 Clusters

```{python plot_clusters3, messages = FALSE}
labels = KMeans(n_clusters = 3, n_init = 10).fit(X).labels_

plot(X, labels)
```

### 4 Clusters

```{python plot_clusters4, messages = FALSE}
np.random.seed(621)
labels = KMeans(n_clusters = 4, n_init = 10).fit(X).labels_

plot(X, labels)
```
:::

I like 4 clusters the best as we are looking at 2 features here, we can form "quadrants" of sorts. Lets apply the labels to the points and see which coaches fell into which categories.

## DBSCAN

```{python, messages = FALSE}

DBSCAN_X = pd.DataFrame(columns = ["eps", "Silhouette"], index = range(10))

for i in range(1,12):
  eps = 0.2*i
  model = DBSCAN(eps=eps).fit(X)
  labels = model.labels_
  try:
    DBSCAN_X.at[i - 1, "Silhouette"] = sklearn.metrics.silhouette_score(X,labels)
  except:
    continue
  DBSCAN_X.at[i - 1, "eps"] = eps
  

plt.clf()
sns.lineplot(data = DBSCAN_X, x = "eps", y = "Silhouette")
plt.show()
```

It looks like the silhouette score is best when the eps is 1.2. Past that there is little to no increase.

::: panel-tabset
### eps = 0.8

```{python}
np.random.seed(621)
labels = DBSCAN(eps = 0.8).fit(X).labels_

plot(X, labels)
```

### eps = 1.0

```{python}
np.random.seed(621)
labels = DBSCAN(eps = 1.0).fit(X).labels_

plot(X, labels)
```

### eps = 1.2

```{python}
np.random.seed(621)
labels = DBSCAN(eps = 1.2).fit(X).labels_

plot(X, labels)
```
:::

## Agglomerative

```{python}
agg_X = pd.DataFrame(columns = ["Clusters", "Silhouette"], index = range(15))

for i in range(1,16):
  model = AgglomerativeClustering(n_clusters=i).fit(X)
  labels = model.labels_
  try:
    agg_X.at[i - 1, "Silhouette"] = sklearn.metrics.silhouette_score(X,labels)
  except:
    continue
  agg_X.at[i - 1, "Clusters"] = i
  

plt.clf()
sns.lineplot(data = agg_X, x = "Clusters", y = "Silhouette")
plt.show()
```

The best number of clusters looks to be around 8, we will try 4, 8, and 12.

::: panel-tabset
### 4 Clusters

```{python, messages = FALSE}
labels = AgglomerativeClustering(n_clusters = 4).fit(X).labels_

plot(X, labels)
```

### 8 Clusters

```{python, messages = FALSE}
labels = AgglomerativeClustering(n_clusters = 8).fit(X).labels_

plot(X, labels)
```

### 12 Clusters

```{python, messages = FALSE}
np.random.seed(621)
labels = AgglomerativeClustering(n_clusters = 12).fit(X).labels_

plot(X, labels)
```

8 and 12 seem to be too many clusters, and 4 looks very similar to the result from K-Means clustering so we will continue using that result.
:::

# Labelling Coaches

The four categories I am using to describe coaches here are "Passive", "Aggresive", "Impulsive", and "Strategic". This can be seen on the plot below. Passive coaches tend to not go for it when they should not be going for it, but also they fail to go for it when they should. Aggresive coaches are the opposite, they usually go for it in both good and bad situations. Implusive coaches do not go for it when they should and do go for it when they should not. Strategic coaches are the opposite, usually making good decisions in all scenarios. This gives us an interesting way to compare coaches and start to create "archetypes" of different coaches based on their behavior in these situations.

```{python add_labels, include = FALSE}
np.random.seed(621)
labels = KMeans(n_clusters = 4, n_init = 10).fit(X).labels_
labels = pd.DataFrame(labels)
```

```{r plot_with_labels, warning = FALSE}
labels <- py$labels

count <- fourth_decisions %>%
  group_by(coach) %>%
  summarize(count = sum(count)) %>%
  ungroup()

fourth_by_coach <- fourth_by_coach %>%
  mutate(count = count$count)
  

category_labels <- c("Passive", "Aggresive", "Impulsive", "Strategic")

fourth_by_coach <- cbind(fourth_by_coach, labels)

fourth_by_coach <- fourth_by_coach %>%
  rename(labels = "0") %>%
  mutate(labels = labels + 1)

fourth_by_coach$labels <- category_labels[fourth_by_coach$labels]

hover <- paste(fourth_by_coach$coach,
             "<br>Count: ", fourth_by_coach$count)

p1 <- ggplot(data = fourth_by_coach) +
  geom_point(aes(x = should_go, y = shouldnt_go, color = labels, text = hover)) +
  xlab("Should Go Correct Rate") +
  ylab("Shouldn't Go Correct Rate") +
  scale_x_continuous(breaks = seq(0.15, 0.55, by = 0.05)) +
  scale_y_continuous(breaks = seq(0.85, 1, by = 0.01)) +
  scale_color_brewer(palette = "Dark2")

ggplotly(p1)  %>%
  layout(legend = list(title = list(text = "Category")))

```
