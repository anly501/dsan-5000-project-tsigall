{"title":"Record Data","markdown":{"yaml":{"title":"Record Data","editor_options":{"chunk_output_type":"inline"}},"headingText":"The Task at Hand","containsRefs":false,"markdown":"\n\n```{r, include = FALSE}\nlibrary(tidyverse)\nlibrary(kableExtra)\n```\n\n\nAs we are looking at coach decision making, could we develop a model to effectively predict when coaches will and will not go for it on 4th down? In other words, what properties of the (coach, situation) pair will effectively predict whether or not a coach decides to go for it on 4th down? Doing this will give valuable insights into the decision-making process as understanding what situations each coach decides to go for it or not allows us to compare philosophies between coaches.\n\n## Prepare Data\n\n```{r load_data, echo = FALSE}\nload(\"raw_nfl.Rdata\")\ndf <- raw_data %>%\n    filter(!is.na(go_boost) & !is.na(go)) %>%\n    select(season, home_coach, away_coach, posteam, defteam, posteam_type, game_half, half_seconds_remaining, ydstogo, yardline_100, posteam_score, defteam_score, posteam, go_boost, go, epa, wp_fail, wp_succeed, wp, fg_make_prob, miss_fg_wp, make_fg_wp, punt_wp)\ndf <- df %>%\n  mutate(coach = if_else(posteam_type == \"home\", home_coach, away_coach),\n         home_coach = coach,\n         score_diff = posteam_score - defteam_score,\n         go = if_else(go == 100, 1, 0),\n         game_half = if_else(game_half == \"Half1\", 1, 2)) %>%\n  select(-coach,\n         -away_coach,\n         -season,\n         -posteam,\n         -defteam,\n         -posteam_type,\n         -epa) %>%\n  rename(coach = home_coach) %>%\n  select(-go, everything())\nkable(head(df))\nwrite_csv(df, file = \"df.csv\")\n```\n\nFeatures for this model include: game_half, half_seconds_remaining, ydstogo, yardline_100, posteam_score, defteam_score, go_boost, wp_fail, wp_succeed, wp, fg_make_prob, miss_fg_wp, make_fg_wp, punt_wp.\n\nWe want to look at the decision making process of each individual coach. To start, lets look at the process of the coach that has stayed on the same team and had the most 4th down situations over the past 8 NFL seasons: Bill Belichick.\n\n```{r most_4ths, echo = FALSE}\nkable(head(df %>%\n  group_by(coach) %>%\n  summarize(count = n()) %>%\n  arrange(desc(count))))\n```\n\n# Machine Learning before Feature Selection\n\nIt is important to establish benchmarks by which to judge our models by before we start to build them. This is done by creating basic models that predict through randomly guessing. The purpose of this is to compare our actual models to these - if they cannot outperform a model that predicts through random guessing then we have a problem.\n\n```{python, include = FALSE}\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\npd.set_option('display.float_format', '{:.3f}'.format)\n```\n\n## Random Guessing\n\n```{python set_seed, include = FALSE}\nrng = np.random.default_rng(621)\ndf = pd.read_csv(\"df.csv\")\nbelichick = df[df['coach'] == 'Bill Belichick']\nbelichick = belichick.drop(columns =['coach', 'punt_wp'])\nbelichick['random_guess'] = rng.choice([0,1], len(belichick))\n```\n\n```{python metric, echo = FALSE}\nf1_random = f1_score(belichick['go'], belichick['random_guess'])\naccuracy_random = accuracy_score(belichick['go'], belichick['random_guess'])\nmetrics = pd.DataFrame({\"Metric\" : [\"F1 Score\", \"Accuracy Score\"],\n  \"Random Guessing\" : [f1_random, accuracy_random]})\nmetrics\n```\n\n## Most Frequent Label\n\n```{python}\nmost_frequent_label = int(belichick['go'].mode())\nbelichick['frequent'] = most_frequent_label\nf1_random = f1_score(belichick['go'], belichick['frequent'])\naccuracy_random = accuracy_score(belichick['go'], belichick['frequent'])\nmetrics = pd.DataFrame({\"Metric\" : [\"F1 Score\", \"Accuracy Score\"],\n  \"Random Guessing\" : [f1_random, accuracy_random]})\nmetrics\n```\n\n# Train and Test Model\n\n```{python, include = FALSE}\nfeature_matrix = belichick.drop(columns = ['go', 'random_guess'])\nlabel_vec = belichick[['go']]\n\nX_train, X_test, y_train, y_test = train_test_split(feature_matrix, label_vec, test_size=0.2, random_state=621)\nmodel = GaussianNB()\nmodel.fit(X_train, np.ravel(y_train))\ntest_predictions = model.predict(X_test)\n\nf1_gaussianNB = f1_score(y_true = y_test, y_pred = test_predictions)\naccuracy_gaussianNB = accuracy_score(y_true = y_test, y_pred = test_predictions)\n```\n\n```{python metrics, echo = FALSE}\nmetrics[\"GaussianNB\"] = [f1_gaussianNB, accuracy_gaussianNB]\nmetrics\n```\n\nThis model is a clear improvement over random guessing, meaning we have a legitimate model. We can now attempt to improve the model through feature selection, and then move on to creating a model for each coach.\n\n# Feature Selection\n\n```{python}\ndef merit(x,y,correlation=\"pearson\"):\n    # x=matrix of features \n    # y=matrix (or vector) of targets \n    # correlation=\"pearson\" or \"spearman\"\n    k = len(x)\n    feature = pd.DataFrame(x)\n    target = pd.DataFrame(y)\n\n    f_corr = feature.corr(method=correlation)\n\n    if(correlation == \"pearson\"):\n        f_t_corr = np.corrcoef(feature, target, rowvar=False)\n    else:\n        f_t_corr =  feature.apply(lambda col: col.corr(target, method = \"spearman\"))\n        \n    rho_xx = f_t_corr.mean()\n    mask = np.triu(np.ones_like(f_corr), k=1)\n    rho_xy = f_corr[pd.DataFrame(mask) == 1].mean().mean()\n    return k*np.absolute(rho_xy)/(np.sqrt(k+k*(k+1)*np.absolute(rho_xx)))\n```\n\n```{python}\nx = feature_matrix.to_numpy()\ny = label_vec.to_numpy()\n\ndef explore_data(x,y,iplot=True):\n\n    #PRINT SHAPE\n    print(x.shape)\n    print(y.shape)\n\n    #COMPUTE MERIT \n    print(\"merit =\",merit(x,y,correlation=\"spearman\")); \n    print(\"merit =\",merit(x,y,correlation=\"pearson\"))\n    \n    #PLOT\n    if (iplot):\n        sns.pairplot(pd.DataFrame(np.hstack((x,y.reshape(y.shape[0],1)))))\n        plt.show()\n\n# TEST YOUR CODE ABOVE\nexplore_data(x,y, iplot = False)\n```\n\n## Iteration\n\n```{python}\nimport itertools\n\ndef maximize_CFS(x,y):\n     k = x.shape[1]\n     max_merit = 0\n     list1 = [*range(0, k)]; #print(list1)\n     for L in range(2, len(list1) + 1):\n          for subset in itertools.combinations(list1, L):\n               m = merit(x[: ,subset], y)\n               if(m > max_merit):\n                    max_merit = m\n                    optimal_subset = subset\n                    print(\"found new max: \", max_merit, \"optimal features = \", list(subset))\n     return(x[:, optimal_subset])\n   \nx_opt=maximize_CFS(x,y)\nexplore_data(x_opt, y, iplot = False)\nfeatures_opt = pd.DataFrame(x_opt, columns = feature_matrix.columns[[3,11]])\n```\n\n## Merit Score\n\n$$ \\mathrm {Merit} _{S_{k}}={\\frac {k|{\\overline {r_{cf}}|}}{\\sqrt {k+k(k-1)|{\\overline {r_{ff}}}|}}}  $$\n\n```{python}\nfrom scipy.stats import spearmanr\n\n#compute base merit score s_2\n\nincluded_vars_s2 = ['half_seconds_remaining','yardline_100']\nincluded_vars_df = feature_matrix[included_vars_s2].copy()\ntime_yardline_corr = spearmanr(included_vars_df['half_seconds_remaining'], \n                                  included_vars_df['yardline_100']).statistic\ntime_yardline_corr\n\ntime_go_corr = spearmanr(included_vars_df['half_seconds_remaining'], label_vec).statistic\ntime_go_corr\n\nyardline_go_corr = spearmanr(included_vars_df['yardline_100'], label_vec).statistic\nyardline_go_corr\n\nk = 2\n\n# since we only have two features there is only one correlation value, so this mean is just the one value\nmean_xx_corr = time_yardline_corr #r_{xx}\n\n# mean_xy_corr is the mean of the two feature/label correlations from above\nmean_xy_corr = np.mean([time_go_corr, yardline_go_corr]) #r_{xy}\n\nprint(f\"Number of Features: {k}\")\nmerit_score_numer = k * np.absolute(mean_xy_corr)\nmerit_score_denom = np.sqrt(k + k * (k + 1) * np.absolute(mean_xx_corr))\nmerit_score_s2 = merit_score_numer / merit_score_denom\n```\n\n```{python}\ndef compute_mean_xx_corr(x_df):\n  df_colnames = x_df.columns\n  # This will contain our final set of x<->x correlations\n  xx_corrs = []\n  # Now we use itertools to iterate over all possible *pairs* of\n  # elements from df_cols\n  df_colname_pairs = itertools.combinations(df_colnames, 2)\n  for colname1, colname2 in df_colname_pairs:\n    # Extract the first column we're considering\n    col1 = x_df[colname1]\n    # Extract the second column\n    col2 = x_df[colname2]\n    # And compute the correlation\n    xx_pair_corr = spearmanr(col1, col2).statistic\n    xx_corrs.append(xx_pair_corr)\n  # And now that the loop has finished running, we can return the **mean**\n  # of the correlation values we've accumulated in the `xx_corrs` list\n  return np.mean(xx_corrs)\n\ndef compute_mean_xy_corr(x_df, y_vec):\n  df_colnames = x_df.columns\n  xy_corrs = []\n  for colname in df_colnames:\n    x_col = x_df[colname]\n    xy_pair_corr = spearmanr(x_col, y_vec)\n    xy_corrs.append(xy_pair_corr)\n  # And return the mean\n  return np.mean(xy_corrs)\n\nincluded_vars_sp3 = ['half_seconds_remaining','yardline_100', 'score_diff']\nincluded_vars_df = feature_matrix[included_vars_sp3].copy()\nmean_xx_corr = compute_mean_xx_corr(included_vars_df)\nmean_xy_corr = compute_mean_xy_corr(included_vars_df, label_vec)\nmean_xx_corr, mean_xy_corr\n\ndef compute_merit_score(num_features, mean_xx_corr, mean_xy_corr):\n  merit_score_numer = k * np.absolute(mean_xy_corr)\n  merit_score_denom = np.sqrt(k + k * (k + 1) * np.absolute(mean_xx_corr))\n  merit_score = merit_score_numer / merit_score_denom\n  return merit_score\n\nmerit_score_sp3 = compute_merit_score(3, mean_xx_corr, mean_xy_corr)\n\n\nincluded_vars_sp5 = ['half_seconds_remaining','yardline_100', 'score_diff', 'posteam_score', 'defteam_score']\nincluded_vars_df = feature_matrix[included_vars_sp5].copy()\nmean_xx_corr = compute_mean_xx_corr(included_vars_df)\nmean_xy_corr = compute_mean_xy_corr(included_vars_df, label_vec)\nmean_xx_corr, mean_xy_corr\n\nmerit_score_sp5 = compute_merit_score(5, mean_xx_corr, mean_xy_corr)\n\nprint(f\"Merit score (S_2): {merit_score_s2}\")\nprint(f\"Merit score (S'_3): {merit_score_sp3}\")\nprint(f\"Merit score (S'_5): {merit_score_sp5}\")\n```\n\n### Evaluate New Model\n\n```{python}\ncoach = \"Ron Rivera\"\n\ncoach_df = df[df['coach'] == coach]\ncoach_df = coach_df.drop(columns = ['punt_wp'])\n\nfeature_matrix = coach_df.drop(columns = ['go', 'coach'])\nfeature_matrix = feature_matrix.apply(zscore)\nlabel_vec = coach_df[['go']]\n\nfeature_matrix_s3 = feature_matrix[included_vars_sp3].copy()\nX_train, X_test, y_train, y_test = train_test_split(feature_matrix_s3, label_vec, test_size=0.2, random_state=621)\n\nmodel = GaussianNB()\nmodel.fit(X_train, np.ravel(y_train))\ntest_predictions = model.predict(X_test)\n\nfs_clf_f1 = f1_score(y_true = y_test, y_pred = test_predictions)\naccuracy_gaussianNB = accuracy_score(y_true = y_test, y_pred = test_predictions)\n\nprint(f\"Feature Selection Model F1: {fs_clf_f1}\")\nprint(f\"Accuracy: {accuracy_gaussianNB}\")\n```\n\n# Compare Coaches\n\n```{python, output = FALSE}\ndef train_coach_model(coach):\n  coach_df = df[df['coach'] == coach]\n  # coach_df = coach_df[['yardline_100', 'miss_fg_wp', 'go']]\n  coach_df = coach_df.drop(columns = ['punt_wp'])\n  \n  feature_matrix = coach_df.drop(columns = ['go', 'coach'])\n  feature_matrix = feature_matrix.apply(zscore)\n  label_vec = coach_df[['go']]\n\n  X_train, X_test, y_train, y_test = train_test_split(feature_matrix, label_vec, test_size=0.2, random_state=621)\n  model = GaussianNB()\n  model.fit(X_train, np.ravel(y_train))\n  test_predictions = model.predict(X_test)\n\n  f1_gaussianNB = f1_score(y_true = y_test, y_pred = test_predictions)\n  accuracy_gaussianNB = accuracy_score(y_true = y_test, y_pred = test_predictions)\n  metrics[coach] = [f1_gaussianNB, accuracy_gaussianNB]\n  compare_coaches[coach] = abs(model.theta_[1] - model.theta_[0])\n  \n  return model\n```\n\nThis function allows us to easily create a model for any coach we want, and compare the influence each feature has on their decision making. Essentially we are now comparing what coaches take into consideration when they decide to go for it or not.\n\n```{python}\ncompare_coaches = pd.DataFrame({\"Feature\":feature_matrix.columns.to_list()})\n\nstaley_model = train_coach_model(\"Brandon Staley\")\nbelichick_model = train_coach_model(\"Bill Belichick\")\nrivera_model = train_coach_model(\"Ron Rivera\")\nstefanski_model = train_coach_model(\"Kevin Stefanski\")\n\ncompare_coaches = compare_coaches.T.iloc[1:]\ncompare_coaches.rename(columns = dict(zip(compare_coaches.columns, feature_matrix.columns.to_list())), inplace = True)\n\n```\n","srcMarkdownNoYaml":"\n\n```{r, include = FALSE}\nlibrary(tidyverse)\nlibrary(kableExtra)\n```\n\n# The Task at Hand\n\nAs we are looking at coach decision making, could we develop a model to effectively predict when coaches will and will not go for it on 4th down? In other words, what properties of the (coach, situation) pair will effectively predict whether or not a coach decides to go for it on 4th down? Doing this will give valuable insights into the decision-making process as understanding what situations each coach decides to go for it or not allows us to compare philosophies between coaches.\n\n## Prepare Data\n\n```{r load_data, echo = FALSE}\nload(\"raw_nfl.Rdata\")\ndf <- raw_data %>%\n    filter(!is.na(go_boost) & !is.na(go)) %>%\n    select(season, home_coach, away_coach, posteam, defteam, posteam_type, game_half, half_seconds_remaining, ydstogo, yardline_100, posteam_score, defteam_score, posteam, go_boost, go, epa, wp_fail, wp_succeed, wp, fg_make_prob, miss_fg_wp, make_fg_wp, punt_wp)\ndf <- df %>%\n  mutate(coach = if_else(posteam_type == \"home\", home_coach, away_coach),\n         home_coach = coach,\n         score_diff = posteam_score - defteam_score,\n         go = if_else(go == 100, 1, 0),\n         game_half = if_else(game_half == \"Half1\", 1, 2)) %>%\n  select(-coach,\n         -away_coach,\n         -season,\n         -posteam,\n         -defteam,\n         -posteam_type,\n         -epa) %>%\n  rename(coach = home_coach) %>%\n  select(-go, everything())\nkable(head(df))\nwrite_csv(df, file = \"df.csv\")\n```\n\nFeatures for this model include: game_half, half_seconds_remaining, ydstogo, yardline_100, posteam_score, defteam_score, go_boost, wp_fail, wp_succeed, wp, fg_make_prob, miss_fg_wp, make_fg_wp, punt_wp.\n\nWe want to look at the decision making process of each individual coach. To start, lets look at the process of the coach that has stayed on the same team and had the most 4th down situations over the past 8 NFL seasons: Bill Belichick.\n\n```{r most_4ths, echo = FALSE}\nkable(head(df %>%\n  group_by(coach) %>%\n  summarize(count = n()) %>%\n  arrange(desc(count))))\n```\n\n# Machine Learning before Feature Selection\n\nIt is important to establish benchmarks by which to judge our models by before we start to build them. This is done by creating basic models that predict through randomly guessing. The purpose of this is to compare our actual models to these - if they cannot outperform a model that predicts through random guessing then we have a problem.\n\n```{python, include = FALSE}\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\npd.set_option('display.float_format', '{:.3f}'.format)\n```\n\n## Random Guessing\n\n```{python set_seed, include = FALSE}\nrng = np.random.default_rng(621)\ndf = pd.read_csv(\"df.csv\")\nbelichick = df[df['coach'] == 'Bill Belichick']\nbelichick = belichick.drop(columns =['coach', 'punt_wp'])\nbelichick['random_guess'] = rng.choice([0,1], len(belichick))\n```\n\n```{python metric, echo = FALSE}\nf1_random = f1_score(belichick['go'], belichick['random_guess'])\naccuracy_random = accuracy_score(belichick['go'], belichick['random_guess'])\nmetrics = pd.DataFrame({\"Metric\" : [\"F1 Score\", \"Accuracy Score\"],\n  \"Random Guessing\" : [f1_random, accuracy_random]})\nmetrics\n```\n\n## Most Frequent Label\n\n```{python}\nmost_frequent_label = int(belichick['go'].mode())\nbelichick['frequent'] = most_frequent_label\nf1_random = f1_score(belichick['go'], belichick['frequent'])\naccuracy_random = accuracy_score(belichick['go'], belichick['frequent'])\nmetrics = pd.DataFrame({\"Metric\" : [\"F1 Score\", \"Accuracy Score\"],\n  \"Random Guessing\" : [f1_random, accuracy_random]})\nmetrics\n```\n\n# Train and Test Model\n\n```{python, include = FALSE}\nfeature_matrix = belichick.drop(columns = ['go', 'random_guess'])\nlabel_vec = belichick[['go']]\n\nX_train, X_test, y_train, y_test = train_test_split(feature_matrix, label_vec, test_size=0.2, random_state=621)\nmodel = GaussianNB()\nmodel.fit(X_train, np.ravel(y_train))\ntest_predictions = model.predict(X_test)\n\nf1_gaussianNB = f1_score(y_true = y_test, y_pred = test_predictions)\naccuracy_gaussianNB = accuracy_score(y_true = y_test, y_pred = test_predictions)\n```\n\n```{python metrics, echo = FALSE}\nmetrics[\"GaussianNB\"] = [f1_gaussianNB, accuracy_gaussianNB]\nmetrics\n```\n\nThis model is a clear improvement over random guessing, meaning we have a legitimate model. We can now attempt to improve the model through feature selection, and then move on to creating a model for each coach.\n\n# Feature Selection\n\n```{python}\ndef merit(x,y,correlation=\"pearson\"):\n    # x=matrix of features \n    # y=matrix (or vector) of targets \n    # correlation=\"pearson\" or \"spearman\"\n    k = len(x)\n    feature = pd.DataFrame(x)\n    target = pd.DataFrame(y)\n\n    f_corr = feature.corr(method=correlation)\n\n    if(correlation == \"pearson\"):\n        f_t_corr = np.corrcoef(feature, target, rowvar=False)\n    else:\n        f_t_corr =  feature.apply(lambda col: col.corr(target, method = \"spearman\"))\n        \n    rho_xx = f_t_corr.mean()\n    mask = np.triu(np.ones_like(f_corr), k=1)\n    rho_xy = f_corr[pd.DataFrame(mask) == 1].mean().mean()\n    return k*np.absolute(rho_xy)/(np.sqrt(k+k*(k+1)*np.absolute(rho_xx)))\n```\n\n```{python}\nx = feature_matrix.to_numpy()\ny = label_vec.to_numpy()\n\ndef explore_data(x,y,iplot=True):\n\n    #PRINT SHAPE\n    print(x.shape)\n    print(y.shape)\n\n    #COMPUTE MERIT \n    print(\"merit =\",merit(x,y,correlation=\"spearman\")); \n    print(\"merit =\",merit(x,y,correlation=\"pearson\"))\n    \n    #PLOT\n    if (iplot):\n        sns.pairplot(pd.DataFrame(np.hstack((x,y.reshape(y.shape[0],1)))))\n        plt.show()\n\n# TEST YOUR CODE ABOVE\nexplore_data(x,y, iplot = False)\n```\n\n## Iteration\n\n```{python}\nimport itertools\n\ndef maximize_CFS(x,y):\n     k = x.shape[1]\n     max_merit = 0\n     list1 = [*range(0, k)]; #print(list1)\n     for L in range(2, len(list1) + 1):\n          for subset in itertools.combinations(list1, L):\n               m = merit(x[: ,subset], y)\n               if(m > max_merit):\n                    max_merit = m\n                    optimal_subset = subset\n                    print(\"found new max: \", max_merit, \"optimal features = \", list(subset))\n     return(x[:, optimal_subset])\n   \nx_opt=maximize_CFS(x,y)\nexplore_data(x_opt, y, iplot = False)\nfeatures_opt = pd.DataFrame(x_opt, columns = feature_matrix.columns[[3,11]])\n```\n\n## Merit Score\n\n$$ \\mathrm {Merit} _{S_{k}}={\\frac {k|{\\overline {r_{cf}}|}}{\\sqrt {k+k(k-1)|{\\overline {r_{ff}}}|}}}  $$\n\n```{python}\nfrom scipy.stats import spearmanr\n\n#compute base merit score s_2\n\nincluded_vars_s2 = ['half_seconds_remaining','yardline_100']\nincluded_vars_df = feature_matrix[included_vars_s2].copy()\ntime_yardline_corr = spearmanr(included_vars_df['half_seconds_remaining'], \n                                  included_vars_df['yardline_100']).statistic\ntime_yardline_corr\n\ntime_go_corr = spearmanr(included_vars_df['half_seconds_remaining'], label_vec).statistic\ntime_go_corr\n\nyardline_go_corr = spearmanr(included_vars_df['yardline_100'], label_vec).statistic\nyardline_go_corr\n\nk = 2\n\n# since we only have two features there is only one correlation value, so this mean is just the one value\nmean_xx_corr = time_yardline_corr #r_{xx}\n\n# mean_xy_corr is the mean of the two feature/label correlations from above\nmean_xy_corr = np.mean([time_go_corr, yardline_go_corr]) #r_{xy}\n\nprint(f\"Number of Features: {k}\")\nmerit_score_numer = k * np.absolute(mean_xy_corr)\nmerit_score_denom = np.sqrt(k + k * (k + 1) * np.absolute(mean_xx_corr))\nmerit_score_s2 = merit_score_numer / merit_score_denom\n```\n\n```{python}\ndef compute_mean_xx_corr(x_df):\n  df_colnames = x_df.columns\n  # This will contain our final set of x<->x correlations\n  xx_corrs = []\n  # Now we use itertools to iterate over all possible *pairs* of\n  # elements from df_cols\n  df_colname_pairs = itertools.combinations(df_colnames, 2)\n  for colname1, colname2 in df_colname_pairs:\n    # Extract the first column we're considering\n    col1 = x_df[colname1]\n    # Extract the second column\n    col2 = x_df[colname2]\n    # And compute the correlation\n    xx_pair_corr = spearmanr(col1, col2).statistic\n    xx_corrs.append(xx_pair_corr)\n  # And now that the loop has finished running, we can return the **mean**\n  # of the correlation values we've accumulated in the `xx_corrs` list\n  return np.mean(xx_corrs)\n\ndef compute_mean_xy_corr(x_df, y_vec):\n  df_colnames = x_df.columns\n  xy_corrs = []\n  for colname in df_colnames:\n    x_col = x_df[colname]\n    xy_pair_corr = spearmanr(x_col, y_vec)\n    xy_corrs.append(xy_pair_corr)\n  # And return the mean\n  return np.mean(xy_corrs)\n\nincluded_vars_sp3 = ['half_seconds_remaining','yardline_100', 'score_diff']\nincluded_vars_df = feature_matrix[included_vars_sp3].copy()\nmean_xx_corr = compute_mean_xx_corr(included_vars_df)\nmean_xy_corr = compute_mean_xy_corr(included_vars_df, label_vec)\nmean_xx_corr, mean_xy_corr\n\ndef compute_merit_score(num_features, mean_xx_corr, mean_xy_corr):\n  merit_score_numer = k * np.absolute(mean_xy_corr)\n  merit_score_denom = np.sqrt(k + k * (k + 1) * np.absolute(mean_xx_corr))\n  merit_score = merit_score_numer / merit_score_denom\n  return merit_score\n\nmerit_score_sp3 = compute_merit_score(3, mean_xx_corr, mean_xy_corr)\n\n\nincluded_vars_sp5 = ['half_seconds_remaining','yardline_100', 'score_diff', 'posteam_score', 'defteam_score']\nincluded_vars_df = feature_matrix[included_vars_sp5].copy()\nmean_xx_corr = compute_mean_xx_corr(included_vars_df)\nmean_xy_corr = compute_mean_xy_corr(included_vars_df, label_vec)\nmean_xx_corr, mean_xy_corr\n\nmerit_score_sp5 = compute_merit_score(5, mean_xx_corr, mean_xy_corr)\n\nprint(f\"Merit score (S_2): {merit_score_s2}\")\nprint(f\"Merit score (S'_3): {merit_score_sp3}\")\nprint(f\"Merit score (S'_5): {merit_score_sp5}\")\n```\n\n### Evaluate New Model\n\n```{python}\ncoach = \"Ron Rivera\"\n\ncoach_df = df[df['coach'] == coach]\ncoach_df = coach_df.drop(columns = ['punt_wp'])\n\nfeature_matrix = coach_df.drop(columns = ['go', 'coach'])\nfeature_matrix = feature_matrix.apply(zscore)\nlabel_vec = coach_df[['go']]\n\nfeature_matrix_s3 = feature_matrix[included_vars_sp3].copy()\nX_train, X_test, y_train, y_test = train_test_split(feature_matrix_s3, label_vec, test_size=0.2, random_state=621)\n\nmodel = GaussianNB()\nmodel.fit(X_train, np.ravel(y_train))\ntest_predictions = model.predict(X_test)\n\nfs_clf_f1 = f1_score(y_true = y_test, y_pred = test_predictions)\naccuracy_gaussianNB = accuracy_score(y_true = y_test, y_pred = test_predictions)\n\nprint(f\"Feature Selection Model F1: {fs_clf_f1}\")\nprint(f\"Accuracy: {accuracy_gaussianNB}\")\n```\n\n# Compare Coaches\n\n```{python, output = FALSE}\ndef train_coach_model(coach):\n  coach_df = df[df['coach'] == coach]\n  # coach_df = coach_df[['yardline_100', 'miss_fg_wp', 'go']]\n  coach_df = coach_df.drop(columns = ['punt_wp'])\n  \n  feature_matrix = coach_df.drop(columns = ['go', 'coach'])\n  feature_matrix = feature_matrix.apply(zscore)\n  label_vec = coach_df[['go']]\n\n  X_train, X_test, y_train, y_test = train_test_split(feature_matrix, label_vec, test_size=0.2, random_state=621)\n  model = GaussianNB()\n  model.fit(X_train, np.ravel(y_train))\n  test_predictions = model.predict(X_test)\n\n  f1_gaussianNB = f1_score(y_true = y_test, y_pred = test_predictions)\n  accuracy_gaussianNB = accuracy_score(y_true = y_test, y_pred = test_predictions)\n  metrics[coach] = [f1_gaussianNB, accuracy_gaussianNB]\n  compare_coaches[coach] = abs(model.theta_[1] - model.theta_[0])\n  \n  return model\n```\n\nThis function allows us to easily create a model for any coach we want, and compare the influence each feature has on their decision making. Essentially we are now comparing what coaches take into consideration when they decide to go for it or not.\n\n```{python}\ncompare_coaches = pd.DataFrame({\"Feature\":feature_matrix.columns.to_list()})\n\nstaley_model = train_coach_model(\"Brandon Staley\")\nbelichick_model = train_coach_model(\"Bill Belichick\")\nrivera_model = train_coach_model(\"Ron Rivera\")\nstefanski_model = train_coach_model(\"Kevin Stefanski\")\n\ncompare_coaches = compare_coaches.T.iloc[1:]\ncompare_coaches.rename(columns = dict(zip(compare_coaches.columns, feature_matrix.columns.to_list())), inplace = True)\n\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"feature_record.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","editor":"visual","theme":"sandstone","fontsize":"1em","title":"Record Data","editor_options":{"chunk_output_type":"inline"}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}