{"title":"Clustering","markdown":{"yaml":{"title":"Clustering","format":{"html":{"code-fold":true,"code-summary":"Show the code"}}},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n```{r setup, include = FALSE}\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(reticulate)\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n\nAttempting to \"profile\" each coach by how they perform on 4th down may give us some important insights into how they make their decisions. This would be a good fit for an unsupervised learning task, as this is just some exploratory analysis to see if we can extract some trends from this data. We do not know yet what kinds of coaches there may be out there, so lets attempt to group them based on the data we do have.\n\n# Theory\n\n## K-Means\n\nK-Means clustering is a centroid based clustering algorithm that aims to categorize each observation into the nearest cluster out of the total number k. To do this it randomly picks k points, then computes the distance from each point to each observation. Each observation is assigned the centroid it is closest to, then the centroids are recalculated based on these assignments and the process repeats. This repeats until it is stationary.\n\n## DBSCAN\n\nDBSCAN is a density based clustering algorithm that groups points together based on how close they are to their neighbors. Core points are determined, those being ones that have a certain amount of points close by. Other observations are assigned to the same cluster as those core points if they are within a certain distance of that point. The process repeats until stationary. As a result, outliers belong their own clusters as they are not close to any other points.\n\n## Agglomerative\n\nAgglomerative clustering is a hierarchical clustering algorithm that initially treats each point as its own cluster. The distance between each observation is calculated, and the two closest points are determined to be a cluster. They are merged and the distance matrix is updated with one less cluster (as two clusters (points) were just merged). The two closest clusters are then determined and merged, and this repeats until some stopping criterion is met.\n\n# Methods\n\n## Clean Data for Clustering\n\nWe want to select the features to be used during this process, lets try just using two features to start with: `should_go` and `shouldnt_go` from our earlier cleaning process. Lets also group by coach alone and not by coach and season to make it easier to interpret the final output.\n\n```{r clean_data}\nload(\"clean_data.Rdata\")\n\nfourth_by_coach <- fourth_decisions %>% \n  group_by(coach) %>%\n  summarize(should_go = mean(should_go),\n            shouldnt_go = mean(shouldnt_go)) %>%\n  ungroup()\n  \nhead(fourth_by_coach)\n```\n\nLets get an initial look at the data before applying the clustering algorithm.\n\n```{r initial_viz}\nplot_ly(data = fourth_by_coach, x = ~should_go, y = ~shouldnt_go, type = \"scatter\", mode = \"markers\")\n```\n\nObvious clusters have not formed from this, but we will still proceed with the unsupervised learning task.\n\n```{r to_csv, include = FALSE}\nwrite_csv(fourth_by_coach, file = \"fourth_by_coach.csv\")\n```\n\n### Imports\n\n```{python imports}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nimport sklearn\n\nnp.random.seed(621)\n```\n\n```{python read_data, include = FALSE}\ndf = pd.read_csv(\"fourth_by_coach.csv\")\nscaler = StandardScaler()\nX = scaler.fit_transform(df[[\"should_go\", \"shouldnt_go\"]])\n```\n\n### Plotting Function\n\n```{python plot_function}\n# plotting function from lab 4.1 demo\ndef plot(X,color_vector):\n    fig, ax = plt.subplots()\n    ax.scatter(X[:,0], X[:,1],c=color_vector, alpha=0.5) #, c=y\n    ax.set(xlabel='Feature-1 (x_1)', ylabel='Feature-2 (x_2)',\n    title='Cluster data')\n    ax.grid()\n    # fig.savefig(\"test.png\")\n    plt.show()\n```\n\n## K-Means\n\n```{python clustering, messages = FALSE}\nX = np.ascontiguousarray(X)\nk_means_X = pd.DataFrame(columns = [\"Cluster\", \"Inertia\"], index = range(10))\n\nfor i in range(1,11):\n  model = KMeans(n_clusters = i, n_init = 10).fit(X)\n  k_means_X.at[i - 1, \"Inertia\"] = model.inertia_\n  k_means_X.at[i - 1, \"Cluster\"] = i\n  \n\nplt.clf()\nsns.lineplot(data = k_means_X, x = \"Cluster\", y = \"Inertia\")\nplt.show()\n```\n\nUsing the elbow method, the correct number of clusters here seems to be about 3, we will try 2, 3, and 4.\n\n::: panel-tabset\n### 2 Clusters\n\n```{python plot_clusters2, messages = FALSE}\nlabels = KMeans(n_clusters = 2, n_init = 10).fit(X).labels_\n\nplot(X, labels)\n```\n\n### 3 Clusters\n\n```{python plot_clusters3, messages = FALSE}\nlabels = KMeans(n_clusters = 3, n_init = 10).fit(X).labels_\n\nplot(X, labels)\n```\n\n### 4 Clusters\n\n```{python plot_clusters4, messages = FALSE}\nnp.random.seed(621)\nlabels = KMeans(n_clusters = 4, n_init = 10).fit(X).labels_\n\nplot(X, labels)\n```\n:::\n\nI like 4 clusters the best as we are looking at 2 features here, we can form \"quadrants\" of sorts. Lets apply the labels to the points and see which coaches fell into which categories.\n\n## DBSCAN\n\n```{python, messages = FALSE}\n\nDBSCAN_X = pd.DataFrame(columns = [\"eps\", \"Silhouette\"], index = range(10))\n\nfor i in range(1,12):\n  eps = 0.2*i\n  model = DBSCAN(eps=eps).fit(X)\n  labels = model.labels_\n  try:\n    DBSCAN_X.at[i - 1, \"Silhouette\"] = sklearn.metrics.silhouette_score(X,labels)\n  except:\n    continue\n  DBSCAN_X.at[i - 1, \"eps\"] = eps\n  \n\nplt.clf()\nsns.lineplot(data = DBSCAN_X, x = \"eps\", y = \"Silhouette\")\nplt.show()\n```\n\nIt looks like the silhouette score is best when the eps is 1.2. Past that there is little to no increase.\n\n::: panel-tabset\n### eps = 0.8\n\n```{python}\nnp.random.seed(621)\nlabels = DBSCAN(eps = 0.8).fit(X).labels_\n\nplot(X, labels)\n```\n\n### eps = 1.0\n\n```{python}\nnp.random.seed(621)\nlabels = DBSCAN(eps = 1.0).fit(X).labels_\n\nplot(X, labels)\n```\n\n### eps = 1.2\n\n```{python}\nnp.random.seed(621)\nlabels = DBSCAN(eps = 1.2).fit(X).labels_\n\nplot(X, labels)\n```\n:::\n\n## Agglomerative\n\n```{python}\nagg_X = pd.DataFrame(columns = [\"Clusters\", \"Silhouette\"], index = range(15))\n\nfor i in range(1,16):\n  model = AgglomerativeClustering(n_clusters=i).fit(X)\n  labels = model.labels_\n  try:\n    agg_X.at[i - 1, \"Silhouette\"] = sklearn.metrics.silhouette_score(X,labels)\n  except:\n    continue\n  agg_X.at[i - 1, \"Clusters\"] = i\n  \n\nplt.clf()\nsns.lineplot(data = agg_X, x = \"Clusters\", y = \"Silhouette\")\nplt.show()\n```\n\nThe best number of clusters looks to be around 8, we will try 4, 8, and 12.\n\n::: panel-tabset\n### 4 Clusters\n\n```{python, messages = FALSE}\nlabels = AgglomerativeClustering(n_clusters = 4).fit(X).labels_\n\nplot(X, labels)\n```\n\n### 8 Clusters\n\n```{python, messages = FALSE}\nlabels = AgglomerativeClustering(n_clusters = 8).fit(X).labels_\n\nplot(X, labels)\n```\n\n### 12 Clusters\n\n```{python, messages = FALSE}\nnp.random.seed(621)\nlabels = AgglomerativeClustering(n_clusters = 12).fit(X).labels_\n\nplot(X, labels)\n```\n\n8 and 12 seem to be too many clusters, and 4 looks very similar to the result from K-Means clustering so we will continue using that result.\n:::\n\n# Labelling Coaches\n\nThe four categories I am using to describe coaches here are \"Passive\", \"Aggresive\", \"Impulsive\", and \"Strategic\". This can be seen on the plot below. Passive coaches tend to not go for it when they should not be going for it, but also they fail to go for it when they should. Aggresive coaches are the opposite, they usually go for it in both good and bad situations. Implusive coaches do not go for it when they should and do go for it when they should not. Strategic coaches are the opposite, usually making good decisions in all scenarios. This gives us an interesting way to compare coaches and start to create \"archetypes\" of different coaches based on their behavior in these situations.\n\n```{python add_labels, include = FALSE}\nnp.random.seed(621)\nlabels = KMeans(n_clusters = 4, n_init = 10).fit(X).labels_\nlabels = pd.DataFrame(labels)\n```\n\n```{r plot_with_labels, warning = FALSE}\nlabels <- py$labels\n\ncount <- fourth_decisions %>%\n  group_by(coach) %>%\n  summarize(count = sum(count)) %>%\n  ungroup()\n\nfourth_by_coach <- fourth_by_coach %>%\n  mutate(count = count$count)\n  \n\ncategory_labels <- c(\"Passive\", \"Aggresive\", \"Impulsive\", \"Strategic\")\n\nfourth_by_coach <- cbind(fourth_by_coach, labels)\n\nfourth_by_coach <- fourth_by_coach %>%\n  rename(labels = \"0\") %>%\n  mutate(labels = labels + 1)\n\nfourth_by_coach$labels <- category_labels[fourth_by_coach$labels]\n\nhover <- paste(fourth_by_coach$coach,\n             \"<br>Count: \", fourth_by_coach$count)\n\np1 <- ggplot(data = fourth_by_coach) +\n  geom_point(aes(x = should_go, y = shouldnt_go, color = labels, text = hover)) +\n  xlab(\"Should Go Correct Rate\") +\n  ylab(\"Shouldn't Go Correct Rate\") +\n  scale_x_continuous(breaks = seq(0.15, 0.55, by = 0.05)) +\n  scale_y_continuous(breaks = seq(0.85, 1, by = 0.01)) +\n  scale_color_brewer(palette = \"Dark2\")\n\nggplotly(p1)  %>%\n  layout(legend = list(title = list(text = \"Category\")))\n\n```\n","srcMarkdownNoYaml":"\n\n```{r setup, include = FALSE}\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(reticulate)\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n# Introduction\n\nAttempting to \"profile\" each coach by how they perform on 4th down may give us some important insights into how they make their decisions. This would be a good fit for an unsupervised learning task, as this is just some exploratory analysis to see if we can extract some trends from this data. We do not know yet what kinds of coaches there may be out there, so lets attempt to group them based on the data we do have.\n\n# Theory\n\n## K-Means\n\nK-Means clustering is a centroid based clustering algorithm that aims to categorize each observation into the nearest cluster out of the total number k. To do this it randomly picks k points, then computes the distance from each point to each observation. Each observation is assigned the centroid it is closest to, then the centroids are recalculated based on these assignments and the process repeats. This repeats until it is stationary.\n\n## DBSCAN\n\nDBSCAN is a density based clustering algorithm that groups points together based on how close they are to their neighbors. Core points are determined, those being ones that have a certain amount of points close by. Other observations are assigned to the same cluster as those core points if they are within a certain distance of that point. The process repeats until stationary. As a result, outliers belong their own clusters as they are not close to any other points.\n\n## Agglomerative\n\nAgglomerative clustering is a hierarchical clustering algorithm that initially treats each point as its own cluster. The distance between each observation is calculated, and the two closest points are determined to be a cluster. They are merged and the distance matrix is updated with one less cluster (as two clusters (points) were just merged). The two closest clusters are then determined and merged, and this repeats until some stopping criterion is met.\n\n# Methods\n\n## Clean Data for Clustering\n\nWe want to select the features to be used during this process, lets try just using two features to start with: `should_go` and `shouldnt_go` from our earlier cleaning process. Lets also group by coach alone and not by coach and season to make it easier to interpret the final output.\n\n```{r clean_data}\nload(\"clean_data.Rdata\")\n\nfourth_by_coach <- fourth_decisions %>% \n  group_by(coach) %>%\n  summarize(should_go = mean(should_go),\n            shouldnt_go = mean(shouldnt_go)) %>%\n  ungroup()\n  \nhead(fourth_by_coach)\n```\n\nLets get an initial look at the data before applying the clustering algorithm.\n\n```{r initial_viz}\nplot_ly(data = fourth_by_coach, x = ~should_go, y = ~shouldnt_go, type = \"scatter\", mode = \"markers\")\n```\n\nObvious clusters have not formed from this, but we will still proceed with the unsupervised learning task.\n\n```{r to_csv, include = FALSE}\nwrite_csv(fourth_by_coach, file = \"fourth_by_coach.csv\")\n```\n\n### Imports\n\n```{python imports}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nimport sklearn\n\nnp.random.seed(621)\n```\n\n```{python read_data, include = FALSE}\ndf = pd.read_csv(\"fourth_by_coach.csv\")\nscaler = StandardScaler()\nX = scaler.fit_transform(df[[\"should_go\", \"shouldnt_go\"]])\n```\n\n### Plotting Function\n\n```{python plot_function}\n# plotting function from lab 4.1 demo\ndef plot(X,color_vector):\n    fig, ax = plt.subplots()\n    ax.scatter(X[:,0], X[:,1],c=color_vector, alpha=0.5) #, c=y\n    ax.set(xlabel='Feature-1 (x_1)', ylabel='Feature-2 (x_2)',\n    title='Cluster data')\n    ax.grid()\n    # fig.savefig(\"test.png\")\n    plt.show()\n```\n\n## K-Means\n\n```{python clustering, messages = FALSE}\nX = np.ascontiguousarray(X)\nk_means_X = pd.DataFrame(columns = [\"Cluster\", \"Inertia\"], index = range(10))\n\nfor i in range(1,11):\n  model = KMeans(n_clusters = i, n_init = 10).fit(X)\n  k_means_X.at[i - 1, \"Inertia\"] = model.inertia_\n  k_means_X.at[i - 1, \"Cluster\"] = i\n  \n\nplt.clf()\nsns.lineplot(data = k_means_X, x = \"Cluster\", y = \"Inertia\")\nplt.show()\n```\n\nUsing the elbow method, the correct number of clusters here seems to be about 3, we will try 2, 3, and 4.\n\n::: panel-tabset\n### 2 Clusters\n\n```{python plot_clusters2, messages = FALSE}\nlabels = KMeans(n_clusters = 2, n_init = 10).fit(X).labels_\n\nplot(X, labels)\n```\n\n### 3 Clusters\n\n```{python plot_clusters3, messages = FALSE}\nlabels = KMeans(n_clusters = 3, n_init = 10).fit(X).labels_\n\nplot(X, labels)\n```\n\n### 4 Clusters\n\n```{python plot_clusters4, messages = FALSE}\nnp.random.seed(621)\nlabels = KMeans(n_clusters = 4, n_init = 10).fit(X).labels_\n\nplot(X, labels)\n```\n:::\n\nI like 4 clusters the best as we are looking at 2 features here, we can form \"quadrants\" of sorts. Lets apply the labels to the points and see which coaches fell into which categories.\n\n## DBSCAN\n\n```{python, messages = FALSE}\n\nDBSCAN_X = pd.DataFrame(columns = [\"eps\", \"Silhouette\"], index = range(10))\n\nfor i in range(1,12):\n  eps = 0.2*i\n  model = DBSCAN(eps=eps).fit(X)\n  labels = model.labels_\n  try:\n    DBSCAN_X.at[i - 1, \"Silhouette\"] = sklearn.metrics.silhouette_score(X,labels)\n  except:\n    continue\n  DBSCAN_X.at[i - 1, \"eps\"] = eps\n  \n\nplt.clf()\nsns.lineplot(data = DBSCAN_X, x = \"eps\", y = \"Silhouette\")\nplt.show()\n```\n\nIt looks like the silhouette score is best when the eps is 1.2. Past that there is little to no increase.\n\n::: panel-tabset\n### eps = 0.8\n\n```{python}\nnp.random.seed(621)\nlabels = DBSCAN(eps = 0.8).fit(X).labels_\n\nplot(X, labels)\n```\n\n### eps = 1.0\n\n```{python}\nnp.random.seed(621)\nlabels = DBSCAN(eps = 1.0).fit(X).labels_\n\nplot(X, labels)\n```\n\n### eps = 1.2\n\n```{python}\nnp.random.seed(621)\nlabels = DBSCAN(eps = 1.2).fit(X).labels_\n\nplot(X, labels)\n```\n:::\n\n## Agglomerative\n\n```{python}\nagg_X = pd.DataFrame(columns = [\"Clusters\", \"Silhouette\"], index = range(15))\n\nfor i in range(1,16):\n  model = AgglomerativeClustering(n_clusters=i).fit(X)\n  labels = model.labels_\n  try:\n    agg_X.at[i - 1, \"Silhouette\"] = sklearn.metrics.silhouette_score(X,labels)\n  except:\n    continue\n  agg_X.at[i - 1, \"Clusters\"] = i\n  \n\nplt.clf()\nsns.lineplot(data = agg_X, x = \"Clusters\", y = \"Silhouette\")\nplt.show()\n```\n\nThe best number of clusters looks to be around 8, we will try 4, 8, and 12.\n\n::: panel-tabset\n### 4 Clusters\n\n```{python, messages = FALSE}\nlabels = AgglomerativeClustering(n_clusters = 4).fit(X).labels_\n\nplot(X, labels)\n```\n\n### 8 Clusters\n\n```{python, messages = FALSE}\nlabels = AgglomerativeClustering(n_clusters = 8).fit(X).labels_\n\nplot(X, labels)\n```\n\n### 12 Clusters\n\n```{python, messages = FALSE}\nnp.random.seed(621)\nlabels = AgglomerativeClustering(n_clusters = 12).fit(X).labels_\n\nplot(X, labels)\n```\n\n8 and 12 seem to be too many clusters, and 4 looks very similar to the result from K-Means clustering so we will continue using that result.\n:::\n\n# Labelling Coaches\n\nThe four categories I am using to describe coaches here are \"Passive\", \"Aggresive\", \"Impulsive\", and \"Strategic\". This can be seen on the plot below. Passive coaches tend to not go for it when they should not be going for it, but also they fail to go for it when they should. Aggresive coaches are the opposite, they usually go for it in both good and bad situations. Implusive coaches do not go for it when they should and do go for it when they should not. Strategic coaches are the opposite, usually making good decisions in all scenarios. This gives us an interesting way to compare coaches and start to create \"archetypes\" of different coaches based on their behavior in these situations.\n\n```{python add_labels, include = FALSE}\nnp.random.seed(621)\nlabels = KMeans(n_clusters = 4, n_init = 10).fit(X).labels_\nlabels = pd.DataFrame(labels)\n```\n\n```{r plot_with_labels, warning = FALSE}\nlabels <- py$labels\n\ncount <- fourth_decisions %>%\n  group_by(coach) %>%\n  summarize(count = sum(count)) %>%\n  ungroup()\n\nfourth_by_coach <- fourth_by_coach %>%\n  mutate(count = count$count)\n  \n\ncategory_labels <- c(\"Passive\", \"Aggresive\", \"Impulsive\", \"Strategic\")\n\nfourth_by_coach <- cbind(fourth_by_coach, labels)\n\nfourth_by_coach <- fourth_by_coach %>%\n  rename(labels = \"0\") %>%\n  mutate(labels = labels + 1)\n\nfourth_by_coach$labels <- category_labels[fourth_by_coach$labels]\n\nhover <- paste(fourth_by_coach$coach,\n             \"<br>Count: \", fourth_by_coach$count)\n\np1 <- ggplot(data = fourth_by_coach) +\n  geom_point(aes(x = should_go, y = shouldnt_go, color = labels, text = hover)) +\n  xlab(\"Should Go Correct Rate\") +\n  ylab(\"Shouldn't Go Correct Rate\") +\n  scale_x_continuous(breaks = seq(0.15, 0.55, by = 0.05)) +\n  scale_y_continuous(breaks = seq(0.85, 1, by = 0.01)) +\n  scale_color_brewer(palette = \"Dark2\")\n\nggplotly(p1)  %>%\n  layout(legend = list(title = list(text = \"Category\")))\n\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"coach_clustering.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","editor":"visual","theme":"sandstone","fontsize":"1em","title":"Clustering","code-summary":"Show the code"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}