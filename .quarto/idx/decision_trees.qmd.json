{"title":"Decision Trees","markdown":{"yaml":{"title":"Decision Trees","format":{"html":{"code-fold":true,"code-summary":"Show the code"}},"editor_options":{"chunk_output_type":"inline"},"jupyter":"python3"},"headingText":"Python","containsRefs":false,"markdown":"\n\nThe end goal of making these decision trees is to model the decision making process for each coach. To help us better understand the individual trees, we will start by making an \"average\" baseline tree that is built using data from every coach. This classification tree will predict whether or not a coach decides to go for it on a given 4th down scenario. It may help to try this using every 4th down play, as well as with some filters imposed on the data to extract more informative, higher leverage scenarios.\n\n\n```{python}\nimport pandas as pd\nimport seaborn as sns \nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n```\n\n## Basic Data Exploration\n\nWe will take another look at our data just so we are familiar with the parameters we are working with.\n\n```{python}\ndf = pd.read_csv(\"df.csv\").drop([\"coach\", \"Unnamed: 0\"], axis=1)\nsummary = pd.DataFrame({\"dtypes\": df.dtypes, \"min\" : df.min(), \"mean\" : df.mean(), \"max\" : df.max()})\nprint(summary)\n```\n\n```{python}\ncount_0 = sum(df[\"go\"] == 0)\ncount_1 = sum(df[\"go\"] == 1)\ntotal = count_0 + count_1\n\nprint(\"Number of not go points:\", count_0, count_0/total)\nprint(\"Number of go points:\", count_1, count_1/total)\n```\n\n## Isolate Inputs/Output\n\n```{python}\n# Split data into features and target\nX = df.iloc[:,[0, 1, 2, 3, 11]]\nY = df[\"go\"]\n```\n\n```{python}\n# Split data into train and test sets\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=621)\n\n# Check size of train and test splits\nprint(type(x_train), x_train.shape)\nprint(type(y_train), y_train.shape)\nprint(type(x_test), x_test.shape)\nprint(type(y_test), y_test.shape)\n```\n\n## Training the Model\n\n```{python}\nfrom sklearn import tree\nclf = tree.DecisionTreeClassifier(random_state = 621, max_depth = 5)\nmodel = clf.fit(x_train, y_train)\n```\n\n## Check the Model\n\n```{python}\nyp_train = model.predict(x_train)\nyp_test = model.predict(x_test)\n\ndef confusion_plot(y_data, y_pred):\n    cm = confusion_matrix(y_data, y_pred)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                                  display_labels=model.classes_)\n    print(\"ACCURACY: \", accuracy_score(y_data, y_pred))\n    print(\"NEGATIVE RECALL (Y=0): \", recall_score(y_data, y_pred, pos_label=0))\n    print(\"NEGATIVE PRECISION (Y=0): \", precision_score(y_data, y_pred, pos_label=0))\n    print(\"POSITIVE RECALL (Y=1): \", recall_score(y_data, y_pred, pos_label=1))\n    print(\"POSITIVE PRECISION (Y=1): \", precision_score(y_data, y_pred, pos_label=1))\n    print(cm)\n    disp.plot()\n    plt.show()\n    \nprint(\"------TRAINING------\")\nconfusion_plot(y_train,yp_train)\nprint(\"------TEST------\")\nconfusion_plot(y_test,yp_test)\n```\n\nNegative recall and precision are very good, meaning our model is good at predicting when coaches will not go for it, which makes sense. Positive recall and precision, on the other hand, are not nearly as good. It is much harder to predict when coaches will go for it than when they will not go for it.\n\n## Visualize the Tree\n\n```{python}\ndef plot_tree(model,X,Y):\n    fig = plt.figure(figsize=(25,20))\n    _ = tree.plot_tree(model,\n                    feature_names=X.columns,\n                    class_names=[str(class_) for class_ in Y.unique()], \n                        filled=True)\n    plt.show()\n\nplot_tree(model, X, Y)\n```\n\n## Hyper-parameter tuning\n\n```{python}\ntest_results=[]\ntrain_results=[]\n\nfor num_layer in range(1,10):\n    model = tree.DecisionTreeClassifier(max_depth=num_layer)\n    model = model.fit(x_train, y_train)\n\n    yp_train=model.predict(x_train)\n    yp_test=model.predict(x_test)\n\n    # print(y_pred.shape)\n    test_results.append([num_layer,\n    accuracy_score(y_test, yp_test),\n    recall_score(y_test, yp_test,pos_label=0),\n    recall_score(y_test, yp_test,pos_label=1)])\n    \n    train_results.append([num_layer,\n    accuracy_score(y_train, yp_train),\n    recall_score(y_train, yp_train,pos_label=0),\n    recall_score(y_train, yp_train,pos_label=1)])\n\n```\n\n```{python}\nplt.cla()\nax = sns.lineplot(x=np.asarray(test_results)[:,0],\n                y=np.asarray(test_results)[:,1],\n                color=\"red\",\n                marker=\"o\",\n                label=\"test\")\nsns.lineplot(x=np.asarray(train_results)[:,0],\n                y=np.asarray(train_results)[:,1],\n                color=\"blue\",\n                marker=\"o\",\n                label=\"train\")\nax.set(xlabel=\"Number of layers in the decision tree (max_depth)\", ylabel=\"ACCURACY\")\nplt.legend()\nplt.show()\n\nplt.cla()\nax = sns.lineplot(x=np.asarray(test_results)[:,0],\n                y=np.asarray(test_results)[:,3],\n                color=\"red\",\n                marker=\"o\",\n                label=\"test\")\nsns.lineplot(x=np.asarray(train_results)[:,0],\n                y=np.asarray(train_results)[:,3],\n                color=\"blue\",\n                marker=\"o\",\n                label=\"train\")\nax.set(xlabel=\"Number of layers in the decision tree (max_depth)\", ylabel=\"POSITIVE RECALL (Y=1)\")\nplt.legend()\nplt.show()\n```\n\nLooks like 4 layers is the optimal `max_depth` parameter.\n\n","srcMarkdownNoYaml":"\n\nThe end goal of making these decision trees is to model the decision making process for each coach. To help us better understand the individual trees, we will start by making an \"average\" baseline tree that is built using data from every coach. This classification tree will predict whether or not a coach decides to go for it on a given 4th down scenario. It may help to try this using every 4th down play, as well as with some filters imposed on the data to extract more informative, higher leverage scenarios.\n\n# Python\n\n```{python}\nimport pandas as pd\nimport seaborn as sns \nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n```\n\n## Basic Data Exploration\n\nWe will take another look at our data just so we are familiar with the parameters we are working with.\n\n```{python}\ndf = pd.read_csv(\"df.csv\").drop([\"coach\", \"Unnamed: 0\"], axis=1)\nsummary = pd.DataFrame({\"dtypes\": df.dtypes, \"min\" : df.min(), \"mean\" : df.mean(), \"max\" : df.max()})\nprint(summary)\n```\n\n```{python}\ncount_0 = sum(df[\"go\"] == 0)\ncount_1 = sum(df[\"go\"] == 1)\ntotal = count_0 + count_1\n\nprint(\"Number of not go points:\", count_0, count_0/total)\nprint(\"Number of go points:\", count_1, count_1/total)\n```\n\n## Isolate Inputs/Output\n\n```{python}\n# Split data into features and target\nX = df.iloc[:,[0, 1, 2, 3, 11]]\nY = df[\"go\"]\n```\n\n```{python}\n# Split data into train and test sets\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=621)\n\n# Check size of train and test splits\nprint(type(x_train), x_train.shape)\nprint(type(y_train), y_train.shape)\nprint(type(x_test), x_test.shape)\nprint(type(y_test), y_test.shape)\n```\n\n## Training the Model\n\n```{python}\nfrom sklearn import tree\nclf = tree.DecisionTreeClassifier(random_state = 621, max_depth = 5)\nmodel = clf.fit(x_train, y_train)\n```\n\n## Check the Model\n\n```{python}\nyp_train = model.predict(x_train)\nyp_test = model.predict(x_test)\n\ndef confusion_plot(y_data, y_pred):\n    cm = confusion_matrix(y_data, y_pred)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                                  display_labels=model.classes_)\n    print(\"ACCURACY: \", accuracy_score(y_data, y_pred))\n    print(\"NEGATIVE RECALL (Y=0): \", recall_score(y_data, y_pred, pos_label=0))\n    print(\"NEGATIVE PRECISION (Y=0): \", precision_score(y_data, y_pred, pos_label=0))\n    print(\"POSITIVE RECALL (Y=1): \", recall_score(y_data, y_pred, pos_label=1))\n    print(\"POSITIVE PRECISION (Y=1): \", precision_score(y_data, y_pred, pos_label=1))\n    print(cm)\n    disp.plot()\n    plt.show()\n    \nprint(\"------TRAINING------\")\nconfusion_plot(y_train,yp_train)\nprint(\"------TEST------\")\nconfusion_plot(y_test,yp_test)\n```\n\nNegative recall and precision are very good, meaning our model is good at predicting when coaches will not go for it, which makes sense. Positive recall and precision, on the other hand, are not nearly as good. It is much harder to predict when coaches will go for it than when they will not go for it.\n\n## Visualize the Tree\n\n```{python}\ndef plot_tree(model,X,Y):\n    fig = plt.figure(figsize=(25,20))\n    _ = tree.plot_tree(model,\n                    feature_names=X.columns,\n                    class_names=[str(class_) for class_ in Y.unique()], \n                        filled=True)\n    plt.show()\n\nplot_tree(model, X, Y)\n```\n\n## Hyper-parameter tuning\n\n```{python}\ntest_results=[]\ntrain_results=[]\n\nfor num_layer in range(1,10):\n    model = tree.DecisionTreeClassifier(max_depth=num_layer)\n    model = model.fit(x_train, y_train)\n\n    yp_train=model.predict(x_train)\n    yp_test=model.predict(x_test)\n\n    # print(y_pred.shape)\n    test_results.append([num_layer,\n    accuracy_score(y_test, yp_test),\n    recall_score(y_test, yp_test,pos_label=0),\n    recall_score(y_test, yp_test,pos_label=1)])\n    \n    train_results.append([num_layer,\n    accuracy_score(y_train, yp_train),\n    recall_score(y_train, yp_train,pos_label=0),\n    recall_score(y_train, yp_train,pos_label=1)])\n\n```\n\n```{python}\nplt.cla()\nax = sns.lineplot(x=np.asarray(test_results)[:,0],\n                y=np.asarray(test_results)[:,1],\n                color=\"red\",\n                marker=\"o\",\n                label=\"test\")\nsns.lineplot(x=np.asarray(train_results)[:,0],\n                y=np.asarray(train_results)[:,1],\n                color=\"blue\",\n                marker=\"o\",\n                label=\"train\")\nax.set(xlabel=\"Number of layers in the decision tree (max_depth)\", ylabel=\"ACCURACY\")\nplt.legend()\nplt.show()\n\nplt.cla()\nax = sns.lineplot(x=np.asarray(test_results)[:,0],\n                y=np.asarray(test_results)[:,3],\n                color=\"red\",\n                marker=\"o\",\n                label=\"test\")\nsns.lineplot(x=np.asarray(train_results)[:,0],\n                y=np.asarray(train_results)[:,3],\n                color=\"blue\",\n                marker=\"o\",\n                label=\"train\")\nax.set(xlabel=\"Number of layers in the decision tree (max_depth)\", ylabel=\"POSITIVE RECALL (Y=1)\")\nplt.legend()\nplt.show()\n```\n\nLooks like 4 layers is the optimal `max_depth` parameter.\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"decision_trees.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","editor":"visual","theme":"sandstone","fontsize":"1em","title":"Decision Trees","editor_options":{"chunk_output_type":"inline"},"jupyter":"python3","code-summary":"Show the code"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}