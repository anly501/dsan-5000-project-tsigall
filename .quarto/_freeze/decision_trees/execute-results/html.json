{
  "hash": "c5fd3cf3fd5c835efb7045e7d3100f73",
  "result": {
    "markdown": "---\ntitle: Decision Trees\nformat:\n  html:\n    code-fold: true\n    code-summary: Show the code\neditor_options:\n  chunk_output_type: inline\n---\n\nThe end goal of making these decision trees is to model the decision making process for each coach. To help us better understand the individual trees, we will start by making an \"average\" baseline tree that is built using data from every coach. This classification tree will predict whether or not a coach decides to go for it on a given 4th down scenario. It may help to try this using every 4th down play, as well as with some filters imposed on the data to extract more informative, higher leverage scenarios.\n\n# Python\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport seaborn as sns \nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n```\n:::\n\n\n## Basic Data Exploration\n\nWe will take another look at our data just so we are familiar with the parameters we are working with.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndf = pd.read_csv(\"df.csv\").drop([\"coach\", \"Unnamed: 0\"], axis=1)\nsummary = pd.DataFrame({\"dtypes\": df.dtypes, \"min\" : df.min(), \"mean\" : df.mean(), \"max\" : df.max()})\nprint(summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                         dtypes        min        mean          max\ngame_half                 int64   1.000000    1.500789     2.000000\nhalf_seconds_remaining    int64   1.000000  813.952714  1789.000000\nydstogo                   int64   1.000000    7.788377    46.000000\nyardline_100              int64   1.000000   49.846885    99.000000\ngo_boost                float64 -45.550569   -0.553344    48.798246\nwp_fail                 float64   0.000000    0.419746     0.999993\nwp_succeed              float64   0.000010    0.546676     1.000000\nwp                      float64   0.000044    0.475292     0.999944\nfg_make_prob            float64   0.000000    0.295375     0.991509\nmiss_fg_wp              float64   0.000000    0.408353     0.999993\nmake_fg_wp              float64   0.000000    0.523719     0.999995\nscore_diff                int64 -49.000000   -1.021367    49.000000\ngo                        int64   0.000000    0.158385     1.000000\n```\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ncount_0 = sum(df[\"go\"] == 0)\ncount_1 = sum(df[\"go\"] == 1)\ntotal = count_0 + count_1\n\nprint(\"Number of not go points:\", count_0, count_0/total)\nprint(\"Number of go points:\", count_1, count_1/total)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of not go points: 23476 0.8416146841614685\nNumber of go points: 4418 0.15838531583853158\n```\n:::\n:::\n\n\n## Isolate Inputs/Output\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Split data into features and target\nX = df.iloc[:,[0, 1, 2, 3, 11]]\nY = df[\"go\"]\n```\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Split data into train and test sets\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=621)\n\n# Check size of train and test splits\nprint(type(x_train), x_train.shape)\nprint(type(y_train), y_train.shape)\nprint(type(x_test), x_test.shape)\nprint(type(y_test), y_test.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'> (22315, 5)\n<class 'pandas.core.series.Series'> (22315,)\n<class 'pandas.core.frame.DataFrame'> (5579, 5)\n<class 'pandas.core.series.Series'> (5579,)\n```\n:::\n:::\n\n\n## Training the Model\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn import tree\nclf = tree.DecisionTreeClassifier(random_state = 621, max_depth = 5)\nmodel = clf.fit(x_train, y_train)\n```\n:::\n\n\n## Check the Model\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nyp_train = model.predict(x_train)\nyp_test = model.predict(x_test)\n\ndef confusion_plot(y_data, y_pred):\n    cm = confusion_matrix(y_data, y_pred)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                                  display_labels=model.classes_)\n    print(\"ACCURACY: \", accuracy_score(y_data, y_pred))\n    print(\"NEGATIVE RECALL (Y=0): \", recall_score(y_data, y_pred, pos_label=0))\n    print(\"NEGATIVE PRECISION (Y=0): \", precision_score(y_data, y_pred, pos_label=0))\n    print(\"POSITIVE RECALL (Y=1): \", recall_score(y_data, y_pred, pos_label=1))\n    print(\"POSITIVE PRECISION (Y=1): \", precision_score(y_data, y_pred, pos_label=1))\n    print(cm)\n    disp.plot()\n    plt.show()\n    \nprint(\"------TRAINING------\")\nconfusion_plot(y_train,yp_train)\nprint(\"------TEST------\")\nconfusion_plot(y_test,yp_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n------TRAINING------\nACCURACY:  0.8993053999551871\nNEGATIVE RECALL (Y=0):  0.9673098389411577\nNEGATIVE PRECISION (Y=0):  0.9177004538577912\nPOSITIVE RECALL (Y=1):  0.5339805825242718\nPOSITIVE PRECISION (Y=1):  0.7525150905432596\n[[18198   615]\n [ 1632  1870]]\n------TEST------\nACCURACY:  0.8949632550636315\nNEGATIVE RECALL (Y=0):  0.9650439631138752\nNEGATIVE PRECISION (Y=0):  0.9140767824497258\nPOSITIVE RECALL (Y=1):  0.5382096069868996\nPOSITIVE PRECISION (Y=1):  0.7515243902439024\n[[4500  163]\n [ 423  493]]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](decision_trees_files/figure-html/cell-8-output-2.png){width=521 height=430}\n:::\n\n::: {.cell-output .cell-output-display}\n![](decision_trees_files/figure-html/cell-8-output-3.png){width=513 height=434}\n:::\n:::\n\n\nNegative recall and precision are very good, meaning our model is good at predicting when coaches will not go for it, which makes sense. Positive recall and precision, on the other hand, are not nearly as good. It is much harder to predict when coaches will go for it than when they will not go for it.\n\n## Visualize the Tree\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ndef plot_tree(model,X,Y):\n    fig = plt.figure(figsize=(25,20))\n    _ = tree.plot_tree(model,\n                    feature_names=X.columns,\n                    class_names=[str(class_) for class_ in Y.unique()], \n                        filled=True)\n    plt.show()\n\nplot_tree(model, X, Y)\n```\n\n::: {.cell-output .cell-output-display}\n![](decision_trees_files/figure-html/cell-9-output-1.png){width=1879 height=1498}\n:::\n:::\n\n\n## Hyper-parameter tuning\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ntest_results=[]\ntrain_results=[]\n\nfor num_layer in range(1,10):\n    model = tree.DecisionTreeClassifier(max_depth=num_layer)\n    model = model.fit(x_train, y_train)\n\n    yp_train=model.predict(x_train)\n    yp_test=model.predict(x_test)\n\n    # print(y_pred.shape)\n    test_results.append([num_layer,\n    accuracy_score(y_test, yp_test),\n    recall_score(y_test, yp_test,pos_label=0),\n    recall_score(y_test, yp_test,pos_label=1)])\n    \n    train_results.append([num_layer,\n    accuracy_score(y_train, yp_train),\n    recall_score(y_train, yp_train,pos_label=0),\n    recall_score(y_train, yp_train,pos_label=1)])\n```\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nplt.cla()\nax = sns.lineplot(x=np.asarray(test_results)[:,0],\n                y=np.asarray(test_results)[:,1],\n                color=\"red\",\n                marker=\"o\",\n                label=\"test\")\nsns.lineplot(x=np.asarray(train_results)[:,0],\n                y=np.asarray(train_results)[:,1],\n                color=\"blue\",\n                marker=\"o\",\n                label=\"train\")\nax.set(xlabel=\"Number of layers in the decision tree (max_depth)\", ylabel=\"ACCURACY\")\nplt.legend()\nplt.show()\n\nplt.cla()\nax = sns.lineplot(x=np.asarray(test_results)[:,0],\n                y=np.asarray(test_results)[:,3],\n                color=\"red\",\n                marker=\"o\",\n                label=\"test\")\nsns.lineplot(x=np.asarray(train_results)[:,0],\n                y=np.asarray(train_results)[:,3],\n                color=\"blue\",\n                marker=\"o\",\n                label=\"train\")\nax.set(xlabel=\"Number of layers in the decision tree (max_depth)\", ylabel=\"POSITIVE RECALL (Y=1)\")\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](decision_trees_files/figure-html/cell-11-output-1.png){width=597 height=429}\n:::\n\n::: {.cell-output .cell-output-display}\n![](decision_trees_files/figure-html/cell-11-output-2.png){width=597 height=429}\n:::\n:::\n\n\nLooks like 4 layers is the optimal `max_depth` parameter.\n\n",
    "supporting": [
      "decision_trees_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}