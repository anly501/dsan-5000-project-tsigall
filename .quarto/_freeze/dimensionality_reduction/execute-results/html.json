{
  "hash": "27731c2bbd3f3220a2a2515c3020e7fa",
  "result": {
    "markdown": "---\ntitle: \"Dimensionality Reduction\"\nbibliography: reference.bib\n---\n\n\n\n\n\n\n# Principal Component Analysis\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nimport json\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nX = r.df\nto_drop = [\"coach\", \"go\"]\nX = X.drop(to_drop, axis = 1)\nY = r.df[\"go\"]\n\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\npca = PCA(n_components = 5).fit(X)\nvariance_ratios = pca.explained_variance_ratio_\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](dimensionality_reduction_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nThe optimal number of principal components to keep is 2, looking at where the \"elbow\" is on this graph of the proporition variance explained by each principal component.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](dimensionality_reduction_files/figure-html/unnamed-chunk-7-3.png){width=672}\n:::\n:::\n\n\nWhen graphing the first and second principal components and coloring each based on whether the coach attempted a conversion on the play (0 being no and 1 being yes), we can see some clustering take place. There is a clear main group of points with most of the green points being in the upper left portion of the cluster, and the red points being in the bottom right. There are more red than green points, which makes sense as there are more 4th down plays where coaches did not go for it than ones where they did. There are also some outliers where points increase in the y-direction up to 8. From this analysis, it is worth looking at those outliers to see why they are so far up there, as well as the points that are on the edge of the border between red and green points. I have drawn an approximate line on the graph for demonstration purposes, just to show the area that I think is worth looking at.\n\n# t-SNE\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.manifold import TSNE\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n::: panel-tabset\n## Perplexity = 5\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](dimensionality_reduction_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n## Perplexity = 10\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](dimensionality_reduction_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n## Perplexity = 20\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](dimensionality_reduction_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n## Perplexity = 30\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](dimensionality_reduction_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n## Perplexity = 40\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](dimensionality_reduction_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n## Perplexity = 50\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](dimensionality_reduction_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n:::\n\nPerplexity levels 20 and 30 seem to do the best job at reducing the dimensions so that similar points are grouped together, as there are clearer areas of green within the larger areas of red. However, going forward I would prefer to use PCA as my chosen method of dimensionality reduction due to the clearer distinction drawn between the two types of points. PCA seems to preserve data structure and information better than t-SNE because of this difference. The visualization capabilities are similar between the two, though it was easier to make a distinction between points in this case, as I only have two categories of points I am looking at. This may be a place that PCA outperforms t-SNE, as there are only two types of points. If there were multiple scenarios being looked at, then t-SNE may perform better. This is worth keeping in mind going forward as there are instances where there are many categories of points that could be looked at.\n",
    "supporting": [
      "dimensionality_reduction_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}